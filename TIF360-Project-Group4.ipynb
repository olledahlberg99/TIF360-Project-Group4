{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjMFMaYjpoHJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import copy\n",
        "from google.colab import files\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.activation import LeakyReLU\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "random.seed(42)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSr--qYdA9s0"
      },
      "outputs": [],
      "source": [
        "#Set up Cuda and folders\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "os.makedirs(\"saved_models\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZkZcoh2pTYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd7ba35-b21a-44b5-cd5c-617b7ebaf74e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_set = torchvision.datasets.CIFAR10(root=\".\", train=True, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soVIObNjLMFu"
      },
      "outputs": [],
      "source": [
        "img_class = 1   # 1 = Cars\n",
        "idx = list()\n",
        "\n",
        "for i in range(len(train_set)):         # getting index of all samples in class\n",
        "  if train_set[i][1] == img_class:\n",
        "    idx.append(i)\n",
        "\n",
        "subset = np.zeros([len(idx),3, 32, 32])\n",
        "for i in range(len(idx)):\n",
        "  subset[i,:,:,:] = train_set[idx[i]][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyx6M2ZfLalE"
      },
      "outputs": [],
      "source": [
        "loader = torch.utils.data.DataLoader(\n",
        "    subset[:4500], batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    subset[4500:], batch_size=12, shuffle = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7bRcruppfbK"
      },
      "outputs": [],
      "source": [
        "def CutOut(images, cutOutSize=[8,8]):\n",
        "    \n",
        "    # Cuts out a portion of size \"cutOutSize\" from the middle of every image in \"images\"\n",
        "    # Cut pixels are set to 1 (white)\n",
        "\n",
        "    images = images.detach().numpy()                                      # Converting torch tensor to np array\n",
        "    imgSize = images.shape\n",
        "    cutImages = copy.deepcopy(images)                                                 \n",
        "    cutOuts = np.zeros([np.size(images, 0), imgSize[1], cutOutSize[0], cutOutSize[1]])\n",
        "\n",
        "    xmin = int(imgSize[2]/2 - cutOutSize[0]/2)\n",
        "    xmax = int(xmin + cutOutSize[0])\n",
        "    ymin = int(imgSize[3]/2 - cutOutSize[1]/2)\n",
        "    ymax = int(ymin + cutOutSize[1])\n",
        "    \n",
        "    for i in range(len(images)):                                                # Cutting images\n",
        "        \n",
        "      cutOuts[i, :, :, :] = images[i, :, xmin:xmax, ymin:ymax]\n",
        "      cutImages[i, :, xmin:xmax, ymin:ymax] = 1\n",
        "\n",
        "    cutImages=torch.from_numpy(cutImages)                                       # Converting np array to torch tensor\n",
        "    cutOuts=torch.from_numpy(cutOuts)\n",
        "    \n",
        "    return [cutImages, cutOuts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQQxcnN-wH88"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()    \n",
        "    self.model = torch.nn.Sequential(\n",
        "        nn.Conv2d(3,64,3,stride = 2, padding = 1), #16x16x64\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(64,128,3,stride = 2, padding = 1), #8x8x128\n",
        "        nn.BatchNorm2d(128,0.8),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(128,256,3,stride = 2, padding = 1),  #4x4x256\n",
        "        nn.BatchNorm2d(256,0.8),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(256,4096,4,stride=1),\n",
        "        nn.ConvTranspose2d(4096,256,4,stride=1, padding = 0), #4x4x256\n",
        "        nn.BatchNorm2d(256,0.8),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(256,128,4,stride=2, padding = 1), #8x8x128\n",
        "        nn.BatchNorm2d(128,0.8),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128,3,3,1,1),\n",
        "        nn.Tanh()\n",
        "      )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxxOevuSpXK8"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(Discriminator,self).__init__()\n",
        "      self.model = nn.Sequential(\n",
        "          nn.Conv2d(3,64,3,1,0),\n",
        "          nn.LeakyReLU(0.2, inplace=True), #8\n",
        "          nn.Conv2d(64,128,3,2,1),\n",
        "          nn.InstanceNorm2d(128),\n",
        "          nn.LeakyReLU(0.2, inplace=True), #4\n",
        "          nn.Conv2d(128,1,3,1,0)\n",
        "      )\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7wKQTuvpuzt"
      },
      "outputs": [],
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "def save_sample(batch):\n",
        "\n",
        "    samples = next(iter(test_loader))    # Generate a new set of samples\n",
        "    masked_samples, cutOut_samples = CutOut(samples)  # Create masked and cutout Samples\n",
        "    samples = samples.type(Tensor)      # Create a tensor out of it\n",
        "    masked_samples = masked_samples.type(Tensor)  # Create a tensor out of it\n",
        "    generated_cutout = generator(masked_samples)  # Generate a cutout\n",
        "    filled_samples = masked_samples.clone()       # Clone the masked images\n",
        "    filled_samples[:, :, 12 : 20, 12 : 20] = generated_cutout # Fill the masked area with the generated cutout\n",
        "    sample = torch.cat((masked_samples.data, filled_samples.data, samples.data), -2) # Create a combination of the different kind of images\n",
        "    save_image(sample, \"images/%d.png\" % batch, nrow=6, normalize=True) # Save the images\n",
        "\n",
        "    \n",
        "\n",
        "adversarial_loss = torch.nn.MSELoss() # MSE loss\n",
        "pixelwise_loss = torch.nn.L1Loss()  # L1 loss\n",
        "\n",
        "\n",
        "generator = Generator() #Initialize Generator\n",
        "discriminator = Discriminator() #Initialize Discord\n",
        "\n",
        "# Initialize Cuda\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "    pixelwise_loss.cuda()\n",
        "\n",
        "# Initialize the weights\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)\n",
        "\n",
        "# Initialize Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.00008, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.00008, betas=(0.5, 0.999))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEm93d4Tpvhm"
      },
      "outputs": [],
      "source": [
        "# Epoch loop\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # Training loop\n",
        "    gen_adv_loss, gen_pixel_loss, disc_loss = 0, 0, 0\n",
        "    tqdm_bar = tqdm(loader, desc=f'Training Epoch {epoch} ', total=int(len(loader)))\n",
        "    for i, imgs in enumerate(tqdm_bar):\n",
        "\n",
        "          masked_imgs, masked_parts = CutOut(imgs)\n",
        "          masked_imgs = masked_imgs.type(Tensor) # Convert into tensor\n",
        "          masked_parts = masked_parts.type(Tensor) # Convert into tensor\n",
        "\n",
        "          # Initialize Generator training\n",
        "          optimizer_G.zero_grad()\n",
        "\n",
        "          # Generate a batch of images\n",
        "          for j in range(len(masked_imgs)):\n",
        "            gen_parts = generator(masked_imgs)\n",
        "\n",
        "          # Generator Loss\n",
        "          real = Tensor(imgs.shape[0],1,1,1).fill_(1.0) # Tensor filled with 1 indicating real cutouts\n",
        "          generator_adv = adversarial_loss(discriminator(gen_parts), real)  # Adversarial loss\n",
        "          generator_pixel = pixelwise_loss(gen_parts, masked_parts) # Pixel wise loss\n",
        "          generator_loss = 0.01 * generator_adv + 0.99 * generator_pixel #Adjusting ratio between Adversarial and pixel wise loss\n",
        "\n",
        "          # Update Generator\n",
        "          generator_loss.backward()\n",
        "          optimizer_G.step()\n",
        "\n",
        "          # Initialize Discriminator training\n",
        "          optimizer_D.zero_grad()\n",
        "\n",
        "          # Discriminator loss\n",
        "          fake = Tensor(imgs.shape[0],1,1,1).fill_(0.0) # Tensor filled with 0 indicating generated cutouts\n",
        "          discriminator_real = adversarial_loss(discriminator(masked_parts), real)  # Discriminator fed with real cutouts\n",
        "          discriminator_fake = adversarial_loss(discriminator(gen_parts.detach()), fake) # Discriminator fed with generated cutouts\n",
        "          discriminator_loss = 0.5 * (discriminator_real + discriminator_fake)  #Adjusting ratio between real and generated cutouts into the discriminator\n",
        "\n",
        "          # Update Discriminator\n",
        "          discriminator_loss.backward()\n",
        "          optimizer_D.step()\n",
        "          \n",
        "          \n",
        "          # TQDM parameters + visualiation\n",
        "          gen_adv_loss += generator_adv.item()\n",
        "          gen_pixel_loss += generator_pixel.item()\n",
        "          disc_loss += discriminator_loss.item()\n",
        "          tqdm_bar.set_postfix(gen_adv_loss=gen_adv_loss/(i+1), gen_pixel_loss=gen_pixel_loss/(i+1), disc_loss=disc_loss/(i+1))\n",
        "          \n",
        "          # Generate and save images every 50th batch\n",
        "          batch = epoch * len(loader) + i\n",
        "          if batch % 50 == 0:\n",
        "              save_sample(batch)\n",
        "          \n",
        "          # Save Model\n",
        "          if i % 50 == 0:          \n",
        "            torch.save (generator.state_dict(), \"saved_models/generator.pth\")\n",
        "            torch.save(discriminator.state_dict(), \"saved_models/discriminator.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp-oCTtfsYAm"
      },
      "outputs": [],
      "source": [
        "#For loading a network\n",
        "generator.load_state_dict(torch.load(\"generator.pth\"))\n",
        "discriminator.load_state_dict(torch.load(\"discriminator.pth\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TIF360_own_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}